# -*- coding: utf-8 -*-
"""Gpt2Medi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sy1aMLN9yIVMPHo0_wg4ynf-um3vZ66c
"""

#!pip install torch torchtext transformers sentencepiece pandas tqdm datasets

from datasets import load_dataset
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_scheduler
from tqdm import tqdm
import time

data = load_dataset("QuyenAnhDE/Diseases_Symptoms")
df = pd.DataFrame([{'Name': item['Name'], 'Symptoms': item['Symptoms']} for item in data['train']])

df['Symptoms'] = df['Symptoms'].apply(
    lambda x: ', '.join([s.strip().capitalize() for s in x.split(',')])
)

df['prompt'] = df.apply(lambda row: f"Symptoms of {row['Name']} are: {row['Symptoms']}.", axis=1)

#Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')
tokenizer.pad_token = tokenizer.eos_token

# Model
model = GPT2LMHeadModel.from_pretrained('distilgpt2')
model.resize_token_embeddings(len(tokenizer))
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Dataset Preparation
class MedicalDataset(Dataset):
    def __init__(self, df, tokenizer, max_length=128):
        self.samples = df['prompt'].tolist()
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.samples[idx],
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        input_ids = encoding['input_ids'].squeeze(0)
        attention_mask = encoding['attention_mask'].squeeze(0)
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': input_ids.clone()
        }

def collate_fn(batch):
    input_ids = torch.stack([item['input_ids'] for item in batch])
    attention_mask = torch.stack([item['attention_mask'] for item in batch])
    labels = torch.stack([item['labels'] for item in batch])
    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'labels': labels
    }

# Train-Validation Split

dataset = MedicalDataset(df, tokenizer)
train_size = int(0.8 * len(dataset))
valid_size = len(dataset) - train_size
train_data, valid_data = random_split(dataset, [train_size, valid_size])

train_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_fn)
valid_loader = DataLoader(valid_data, batch_size=8, collate_fn=collate_fn)

# Train Setup

optimizer = optim.AdamW(model.parameters(), lr=5e-4)
scheduler = get_scheduler("linear", optimizer=optimizer,
                          num_warmup_steps=50,
                          num_training_steps=len(train_loader) * 10)

criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, label_smoothing=0.1)

# Training loop
results = []

for epoch in range(10):
    start_time = time.time()
    model.train()
    train_loss = 0

    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1} Training"):
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        scheduler.step()

        train_loss += loss.item()

    avg_train_loss = train_loss / len(train_loader)

    #Validation
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for batch in tqdm(valid_loader, desc=f"Epoch {epoch+1} Validation"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            val_loss += loss.item()

    avg_val_loss = val_loss / len(valid_loader)
    duration = time.time() - start_time

    print(f"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Time: {duration:.2f}s")
    results.append({
        'epoch': epoch+1,
        'train_loss': avg_train_loss,
        'val_loss': avg_val_loss,
        'duration_sec': duration
    })

# Inference

def generate_symptoms(disease_name):
    prompt = f"Symptoms of {disease_name} are:"
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    output = model.generate(
        input_ids,
        max_length=50,
        num_beams=5,
        early_stopping=True,
        repetition_penalty=1.1
    )
    return tokenizer.decode(output[0], skip_special_tokens=True)

print(generate_symptoms("Ethylene glycol poisoning-1"))

model.save_pretrained('./Gpt2Medi')
tokenizer.save_pretrained('./Gpt2Medi')
